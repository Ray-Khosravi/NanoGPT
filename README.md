# NanoGPT
A custom GPT-style Transformer built entirely from scratch in PyTorch. Features hand-coded attention mechanisms and a custom BPE tokenizer trained on the TinyStories dataset for efficient local inference.

ğğ¢ğ  ğğ¨ğ°ğğ« ğ¢ğ§ ğ’ğ¦ğšğ¥ğ¥ ğğšğœğ¤ğšğ ğğ¬: ğğ®ğ¢ğ¥ğğ¢ğ§ğ  ğš ğ‚ğ®ğ¬ğ­ğ¨ğ¦ ğ’ğ‹ğŒ ğŸğ«ğ¨ğ¦ ğ’ğœğ«ğšğ­ğœğ¡ ğŸš€

In an era obsessed with billion-parameter giants, I decided to go back to first principles to explore the true potential of ğ’ğ¦ğšğ¥ğ¥ ğ‹ğšğ§ğ ğ®ğšğ ğ ğŒğ¨ğğğ¥ğ¬ (ğ’ğ‹ğŒğ¬).

For this project, I wanted to do more than just fine-tune existing weights. I aimed to understand the architecture. I implemented a GPT-style ğ“ğ«ğšğ§ğ¬ğŸğ¨ğ«ğ¦ğğ« for the TinyStories dataset entirely from the ground up in ğğ²ğ“ğ¨ğ«ğœğ¡, without relying on pre-packaged model wrappers.


<img width="1178" height="563" alt="image" src="https://github.com/user-attachments/assets/91891356-39b6-4528-863e-1e15ca3070e1" />


Technical Deep Dive: ğŸ› ï¸ 

Manual Architecture: ğ‡ğšğ§ğ-ğœğ¨ğğğ ğ€ğ­ğ­ğğ§ğ­ğ¢ğ¨ğ§ ğ¦ğğœğ¡ğšğ§ğ¢ğ¬ğ¦ğ¬ ğšğ§ğ ğ…ğğğğ…ğ¨ğ«ğ°ğšğ«ğ ğ¥ğšğ²ğğ«ğ¬. ğ‚ğ®ğ¬ğ­ğ¨ğ¦ ğ“ğ¨ğ¤ğğ§ğ¢ğ³ğšğ­ğ¢ğ¨ğ§: ğğ®ğ¢ğ¥ğ­ ğš ğğğ„ ğ­ğ¨ğ¤ğğ§ğ¢ğ³ğğ« specifically for this dataset. Efficiency: Designed to run inference locally with high speed (as seen in the demo).



https://github.com/user-attachments/assets/9706e25f-64c2-4e22-b190-e2960beae6b4



This project reinforced a key lesson: Architecture comprehension and optimization are just as critical as raw model size. You don't always need a supercomputer to generate creative narratives; sometimes, you just need clean code and the right data.

ğŸ‘‹ Contact & Connect

Ray Khosravi - AI / Machine Learning Engineer

[LinkedIn](https://www.linkedin.com/in/raoufkhosravi/)

raykhosravi1993@gmail.com


